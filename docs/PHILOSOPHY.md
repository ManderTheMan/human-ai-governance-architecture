## SOURCE EXCERPTS (Distilled)

> “Augmented intelligence, not artificial intelligence.”
> 

> “The system should interrupt me before execution to check accuracy, not assume readiness.”
> 

> “No agent or part has authority — authority is granted by context.”
> 

> “Resolution should remove unnecessary motion, not complexity.”
> 

> “The system must protect me from mis-timed action.”
> 

> “Alignment feels like zero friction, not forced effort.”
> 

---

## REFINED & CORRECTED CANONICAL SPEC

### 1. Augmentation Over Automation

The system is designed to **augment** human cognition, not automate it.

Automation without state awareness is unsafe.

Therefore:

- AI may *assist*, *analyze*, *mirror*, or *suggest*
- AI may not *decide*, *execute*, or *override* by default

### 2. Contextual Authority (Core Principle)

No component — human or machine — has inherent authority.

Authority is **granted by context**.

This ensures:

- human sovereignty
- reversible decisions
- no runaway processes
- no hidden execution paths

### 3. Execution as a Privileged Act

Execution is treated as:

- irreversible
- energy-consuming
- optionality-reducing

Therefore, execution requires:

- preserved context
- bounded scope
- explicit consent
- readiness signals

This philosophy directly prevents:

- burnout
- premature commitment
- false urgency
- systemic collapse

### 4. Resolution Without Pressure

Increasing resolution should:

- increase clarity
- reduce uncertainty
- remove unnecessary motion

It must **never**:

- increase obligation
- accelerate pace
- force convergence

High resolution with low pressure is a design invariant.

### 5. Stillness as a Valid State

The system recognizes stillness, pause, and recovery as **productive states**.

A system that cannot rest safely is not aligned with human cognition.

---

## BUILD NOTES, TESTS, & APPLICATIONS (Design Philosophy)

### 1. AI Constitutional Design

Early constitutional clauses implied:

- *The system shall prefer pausing over guessing.*
- *The system shall surface uncertainty rather than conceal it.*
- *The system shall never simulate urgency.*
- *The system shall ask before acting.*

These clauses later map cleanly into:

- AI safety rules
- agent permissions
- tool invocation policies

### 2. Model Selection Implications

Prefer models that:

- reason well with partial data
- explain uncertainty
- support iterative refinement
- tolerate ambiguity

Avoid systems optimized for:

- rapid completion
- aggressive suggestion
- forced confidence

### 3. System-Level Tests

- Introduce ambiguity → system slows, not accelerates
- Remove context → system requests clarification
- Add urgency signal → system checks readiness first

Failure of these tests indicates philosophical drift.
